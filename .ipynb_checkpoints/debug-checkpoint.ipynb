{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c272205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "# from models.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2d8d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804ec05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Resize((32, 32), antialias=False),\n",
    "#     transforms.CenterCrop(224)\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./datasets', train=True, transform=train_transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root='./datasets', train=False, transform=train_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe49bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "img, lab = next(iter(train_dataloader))\n",
    "print(img.shape, lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "418b562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "skip_info = {}\n",
    "\n",
    "def time_encoding2d(dim, h, w, t):\n",
    "    assert dim%2 == 0\n",
    "    pos_emb = torch.zeros(dim, h, w)\n",
    "    h_dim = int(dim/2)\n",
    "    pos_emb[0::2, :, :] = torch.sin(t.view(1, -1, 1).repeat(h_dim, h, w))/torch.pow(10000, torch.arange(0, h_dim)*2/dim).view(-1, 1, 1).repeat(1, h, w)\n",
    "    pos_emb[1::2, :, :] = torch.cos(t.view(1, -1, 1).repeat(h_dim, h, w))/torch.pow(10000, torch.arange(0, h_dim)*2/dim).view(-1, 1, 1).repeat(1, h, w)\n",
    "    \n",
    "    return pos_emb\n",
    "\n",
    "\n",
    "class Sequential(nn.Sequential):\n",
    "    def forward(self, *args):\n",
    "        for module in self._modules.values():\n",
    "            if type(args) == tuple:\n",
    "                args = module(*args)\n",
    "            else:\n",
    "                args = module(args)\n",
    "        return args\n",
    "\n",
    "\n",
    "class PixCNNPP(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_blocks=2, num_res=4, channels=[4, 16, 32, 64, 128, 256, 512], sz=32):\n",
    "        super().__init__()                \n",
    "        self.model = []\n",
    "\n",
    "        self.upscale = Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=4, kernel_size=1, stride=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=4, num_channels=4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.downscale = Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=3, kernel_size=1, stride=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            # nn.GroupNorm(num_groups=3, num_channels=3),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        ## downsample\n",
    "        for res in range(num_res):\n",
    "            for block in range(num_blocks):\n",
    "                if block == 0:\n",
    "                    self.model.append(DownsampleBlock(in_channels=channels[res], out_channels=channels[res+1], idx=None))\n",
    "                elif block == num_blocks-1:\n",
    "                    self.model.append(DownsampleBlock(in_channels=channels[res+1], out_channels=channels[res+1], idx=res))\n",
    "                else:\n",
    "                    self.model.append(DownsampleBlock(in_channels=channels[res+1], out_channels=channels[res+1], idx=None))\n",
    "\n",
    "            self.model.append(ReduceBlock(in_channels=channels[res+1], out_channels=channels[res+1]))\n",
    "            sz /= 2\n",
    "\n",
    "        ## Bottleneck\n",
    "        self.model.append(\n",
    "            Sequential(\n",
    "                DownsampleBlock(in_channels=channels[num_res], out_channels=channels[num_res], idx=None),\n",
    "                DownsampleBlock(in_channels=channels[num_res], out_channels=channels[num_res], idx=None)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## upsample\n",
    "        for res in reversed(range(1, num_res+1)):\n",
    "            self.model.append(IncreaseBlock(in_channels=channels[res], out_channels=channels[res]))\n",
    "            sz *= 2\n",
    "            for block in range(num_blocks):\n",
    "                if block == 0:\n",
    "                    self.model.append(UpsampleBlock(in_channels=channels[res], out_channels=channels[res-1], idx=res-1))\n",
    "                else:\n",
    "                    self.model.append(UpsampleBlock(in_channels=channels[res-1], out_channels=channels[res-1], idx=None))\n",
    "\n",
    "            self.model.append(ReduceBlock(in_channels=channels[res+1], out_channels=channels[res+1]))\n",
    "\n",
    "        self.model = Sequential(*self.model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        global skip_info\n",
    "        skip_info = {}\n",
    "\n",
    "        x = self.upscale(x)\n",
    "        x, _ = self.model(x, torch.tensor([t], requires_grad=False))\n",
    "        x = self.downscale(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "class ReduceBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        print(\"Red\", self.block(x).shape)\n",
    "        return self.block(x), t\n",
    "\n",
    "\n",
    "class IncreaseBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        print(\"Inc\", self.block(x).shape)\n",
    "        z = self.block(x)\n",
    "        return z, t\n",
    "\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, idx=None):\n",
    "        super().__init__()   \n",
    "        \n",
    "        self.idx = idx\n",
    "        global skip_info\n",
    "        \n",
    "        self.block = Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.downsample=False\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample=True\n",
    "            self.downsample_block = Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1),\n",
    "                # nn.Dropout2d(p=0.1),\n",
    "                nn.GroupNorm(num_groups=out_channels, num_channels=out_channels)\n",
    "            )\n",
    "\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        B, C, H, W = x.shape\n",
    "        pos_emb = time_encoding2d(C, H, W, t)\n",
    "        x = x + pos_emb.to(x)\n",
    "        \n",
    "        if self.downsample:\n",
    "            z = self.relu(self.downsample_block(x) + self.block(x))\n",
    "        else:\n",
    "            z = self.relu(x + self.block(x))\n",
    "            \n",
    "        if self.idx is not None:\n",
    "            skip_info[self.idx] = z\n",
    "        \n",
    "        print(\"Down\", z.shape)\n",
    "        \n",
    "        return z, t\n",
    "    \n",
    "    \n",
    "class UpsampleBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, idx=None):\n",
    "        super().__init__()    \n",
    "        \n",
    "        self.idx = idx\n",
    "        in_ch = in_channels\n",
    "        if self.idx is not None:\n",
    "            in_ch = 2*in_channels\n",
    "                                      \n",
    "        self.block = Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_ch, out_channels=out_channels, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
    "            # nn.Dropout2d(p=0.1),\n",
    "            nn.GroupNorm(num_groups=out_channels, num_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.upsample=False\n",
    "        if in_channels != out_channels:\n",
    "            self.upsample=True\n",
    "            self.upsample_block = Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=in_ch, out_channels=out_channels, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
    "                # nn.Dropout2d(p=0.1),\n",
    "                nn.GroupNorm(num_groups=out_channels, num_channels=out_channels)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        global skip_info\n",
    "        \n",
    "#         for key, value in skip_info.items():\n",
    "#             print(key, value.shape)\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        pos_emb = time_encoding2d(C, H, W, t)\n",
    "        x = x + pos_emb.to(x)\n",
    "        \n",
    "        if self.idx is not None:\n",
    "            x = torch.cat((x, skip_info[self.idx]), dim=1)\n",
    "        \n",
    "        if self.upsample:\n",
    "            z = self.relu(self.upsample_block(x) + self.block(x))\n",
    "        else:\n",
    "            z = self.relu(x + self.block(x))\n",
    "        print(\"Up\", z.shape)\n",
    "        \n",
    "        return z, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a5e9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down torch.Size([1, 16, 32, 32])\n",
      "Down torch.Size([1, 16, 32, 32])\n",
      "Red torch.Size([1, 16, 16, 16])\n",
      "Down torch.Size([1, 32, 16, 16])\n",
      "Down torch.Size([1, 32, 16, 16])\n",
      "Red torch.Size([1, 32, 8, 8])\n",
      "Down torch.Size([1, 64, 8, 8])\n",
      "Down torch.Size([1, 64, 8, 8])\n",
      "Red torch.Size([1, 64, 4, 4])\n",
      "Down torch.Size([1, 128, 4, 4])\n",
      "Down torch.Size([1, 128, 4, 4])\n",
      "Red torch.Size([1, 128, 2, 2])\n",
      "Down torch.Size([1, 128, 2, 2])\n",
      "Down torch.Size([1, 128, 2, 2])\n",
      "Inc torch.Size([1, 128, 4, 4])\n",
      "Up torch.Size([1, 64, 4, 4])\n",
      "Up torch.Size([1, 64, 4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 256, 3, 3], expected input[1, 64, 4, 4] to have 256 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PixCNNPP()\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 94\u001b[0m, in \u001b[0;36mPixCNNPP.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     91\u001b[0m skip_info \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupscale(x)\n\u001b[0;32m---> 94\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownscale(x)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         args \u001b[38;5;241m=\u001b[39m module(args)\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 114\u001b[0m, in \u001b[0;36mReduceBlock.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x), t\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         args \u001b[38;5;241m=\u001b[39m module(args)\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/hpc/share/balasuri/research_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 256, 3, 3], expected input[1, 64, 4, 4] to have 256 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "img, lab = next(iter(train_dataloader))\n",
    "model = PixCNNPP()\n",
    "t = 0\n",
    "\n",
    "out = model(img, t)\n",
    "print(out.shape)\n",
    "\n",
    "# model.load_state_dict(torch.load('./weights/model_40.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## INFERENCE\n",
    "\n",
    "L2_loss = torch.nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    maxT = 1000\n",
    "    beta_t = torch.arange(0, 2000, 2000/maxT)/(1e5)\n",
    "    alpha_bar_t = torch.ones(beta_t.shape)\n",
    "    for i in range(beta_t.shape[0]):\n",
    "        if i == 0:\n",
    "            alpha_bar_t[i] = 1-beta_t[i]\n",
    "        else:\n",
    "            alpha_bar_t[i] = alpha_bar_t[i-1] * (1-beta_t[i])\n",
    "    img, lab = next(iter(train_dataloader))\n",
    "    xt = torch.normal(torch.zeros(img.shape), torch.ones(img.shape))\n",
    "    \n",
    "    for t in range(maxT-1, 0, -1):\n",
    "        epsilon_theta = model(xt, t)    \n",
    "        if t>0:\n",
    "            z = torch.normal(torch.zeros(img.shape), torch.ones(img.shape))\n",
    "        else:\n",
    "            z = torch.zeros(img.shape)\n",
    "            \n",
    "        xt = (xt - (1-beta_t[t])/torch.sqrt(1-alpha_bar_t[t]) * epsilon_theta)/torch.sqrt(1-beta_t[t]) + torch.sqrt(beta_t[t])*z\n",
    "        xt = xt - torch.min(torch.min(xt, dim=3, keepdim=True).values, dim=2, keepdim=True).values\n",
    "        xt = xt / torch.max(torch.max(xt, dim=3, keepdim=True).values, dim=2, keepdim=True).values\n",
    "        xt = xt*2 - 1\n",
    "        \n",
    "        print(t)\n",
    "        img_show = (xt + 1)/2\n",
    "        plt.imshow(img_show[0].permute(1, 2, 0))\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxT = 200\n",
    "# # L2_loss = torch.nn.MSELoss()\n",
    "# beta_t = torch.arange(0, 1000, 1000/maxT)/(1e4)\n",
    "# print(beta_t)\n",
    "# alpha_bar_t = 1\n",
    "\n",
    "# img, lab = next(iter(train_dataloader))\n",
    "# img = 2*img-1\n",
    "\n",
    "# plt.imshow(((img[0]+1)/2).permute(1, 2, 0))\n",
    "# plt.show()\n",
    "\n",
    "# for t in range(1, maxT):\n",
    "#     alpha_bar_t = (1-beta_t[t]) * alpha_bar_t\n",
    "    \n",
    "#     if t %10 == 0:\n",
    "#         print(t)\n",
    "#         ## forward process\n",
    "#         epsilon = torch.normal(torch.zeros(img.shape), torch.sqrt((1-alpha_bar_t)).repeat(img.shape))\n",
    "#         img_t = torch.sqrt(alpha_bar_t)*img + epsilon\n",
    "\n",
    "#         plt.imshow(((img_t[0]+1)/2).permute(1, 2, 0))\n",
    "#         plt.show()\n",
    "\n",
    "# #         ## reverse estimation\n",
    "# #         epsilon_theta = model(img_t, t)    \n",
    "# #         L2_loss(epsilon_theta, epsilon)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b91df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b5a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070e855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef02df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "gx, gy = torch.meshgrid(a, b)\n",
    "torch.tensor(list(zip(gx.flatten(), gy.flatten())))\n",
    "# print(gx, gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = nn.Sequential(nn.Conv2d(1, 2, 3),\n",
    "                   nn.Linear(4, 5))\n",
    "\n",
    "for mod in blk._modules.values():\n",
    "    print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(*args, **kwargs):\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "    \n",
    "func(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (50,)\n",
    "print(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.choice(np.arange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4,5,6]])\n",
    "torch.min(a, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503cca54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
